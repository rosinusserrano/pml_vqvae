The following sections will discuss the results of our baseline experiments, further ideas for the project, and real-world applications of the project.

\subsection{Baseline Experiments}\label{subsec:baseline-results}
    We present some results of our baseline training runs. We did an examplarily training for 10 epochs on a subset of ImageNet to see that it converges. The subset contained 100.000 images with 1000 samples per class. See ... for the full configuration details.
    
    \subsubsection{Autoencoder}\label{subsubsec:autoencoder}
        We were able to show that our implementation of an \ac{ae} is able to reconstruct images from the ImageNet dataset. W

        For this trainig, we used a latent dimension of 32x32x2 pixels, which means 4 bits per pixel for an 128x128x3 input images. That is a compression rate of 4,7 \% compared to the original image size.

        We are surprised by the results of our basic \ac{ae} and its capability to reconstruct images. Even with a realitively small latent dimension and only 10 epochs of training it is able to reconstruct images with a satifiying human percetiple quality.
        
        

\subsubsection{Class Differences}\label{subsubsec:class-differences}
\subsection{Further Ideas for the Project}\label{subsec:further-ideas}
\subsection{Real World Use of the Datasets}\label{subsec:real-world-applications}
\ac{cifar} and ImageNet have been foundational datasets for over a decade and a half.
Over time, their primary use-case has shifted from being cutting-edge datasets for training state-of-the-art models
to serving as a base for experiments and benchmarks.

The prominent role of ImageNet in its early days is evident when looking at AlexNet~\cite{AlexNet}.
The groundbreaking paper on training a \ac{cnn} on ImageNet using GPUs has been cited an extraordinary 166 thousand
times as of the time of this report and is regarded as one of the pivotal moments in the rise of \ac{dnn}.

Today, the most recent text-to-image generation models rely on much larger datasets, often containing billions of samples.
A notable example is LAION-5B, as described by~\cite{laion5b}, which is utilized in models such as Stable
Diffusion, based on the work of~\cite{stable_diff}.
However, upon closer examination of this paper, the ongoing significance of ImageNet becomes evident.
It can be observed that it is used as a dataset for training experimental networks, where using larger, modern
datasets would be prohibitively expensive.
Additionally, metrics such as the \ac{fid}, discussed in Section~\ref{sec:evaluation-metrics}, are calculated using
ImageNet, further highlighting its continued importance in research.

Similar trends can be observed for CIFAR-10.
However, its low resolution limits its applicability for benchmarking modern approaches to image generation.

Based on these observations, while we expect our final model to produce some believable results, we do not anticipate
achieving the level of visual fidelity demonstrated by state-of-the-art image generation models.

\vskip 1em
\textbf{A Note on Overfitting:}
Overfitting refers to the phenomenon where a model starts to memorize the training data instead of learning useful
features, in turn not generalizing well to unseen data.

While theoretically, this may very well be an issue with \ac{ae} and \ac{vae}, we have not experienced this happening
yet with either when training with adequate training-set sizes.
This is likely due to the large dataset sizes we use and the inductive bias of the convolution operation~
\cite{citationNeeded}.