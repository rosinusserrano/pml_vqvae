The following sections will discuss the results of our baseline experiments, further ideas for the project, and real-world applications of the project.

\subsection{Baseline Experiments}\label{subsec:baseline-results}
    We present some results of our baseline training runs. We did an examplarily training for 10 epochs on a subset of ImageNet to see that it converges. The subset contained 100.000 images with 1000 samples per class. See ... for the full configuration details.

    \subsubsection{Autoencoder}\label{subsubsec:autoencoder}
        We were able to show that our implementation of an \ac{ae} is able to reconstruct images from the ImageNet dataset. W

        For this trainig, we used a latent dimension of 32x32x2 pixels, which means 4 bits per pixel for an 128x128x3 input images. That is a compression rate of 4,7 \% compared to the original image size.

        We are surprised by the results of our basic \ac{ae} and its capability to reconstruct images. Even with a realitively small latent dimension and only 10 epochs of training it is able to reconstruct images with a satifiying human percetiple quality.
        

\subsubsection{Class Differences}\label{subsubsec:class-differences}

\subsection{Real World Use of the Datasets}\label{subsec:real-world-applications}
\ac{cifar} and ImageNet have been foundational datasets for over a decade and a half.
Over time, their primary use-case has shifted from being cutting-edge datasets for training state-of-the-art models
to serving as a base for experiments and benchmarks.

The prominent role of ImageNet in its early days is evident when looking at AlexNet~\cite{AlexNet}.
The groundbreaking paper on training a \ac{cnn} on ImageNet using GPUs has been cited an extraordinary 166 thousand
times as of the time of this report and is regarded as one of the pivotal moments in the rise of \ac{dnn}.

Today, the most recent text-to-image generation models rely on much larger datasets, often containing billions of samples.
A notable example is LAION-5B, as described by~\cite{laion5b}, which is utilized in models such as Stable
Diffusion, based on the work of~\cite{stable_diff}.
However, upon closer examination of this paper, the ongoing significance of ImageNet becomes evident.
It can be observed that it is used as a dataset for training experimental networks, where using larger, modern
datasets would be prohibitively expensive.
Additionally, metrics such as the \ac{fid}, discussed in Section~\ref{sec:evaluation-metrics}, are calculated using
ImageNet, further highlighting its continued importance in research.

Similar trends can be observed for CIFAR-10.
However, its low resolution limits its applicability for benchmarking modern approaches to image generation.

Based on these observations, while we expect our final model to produce some believable results, we do not anticipate
achieving the level of visual fidelity demonstrated by state-of-the-art image generation models.

\subsection{Dataset}
    As the results of our exemplary training runs of only a small subset of training data show, our models already exhibit promising results. We are confident that with the full ImageNet dataset, we will be able to train our models to a better perfomance, at least in terms of reconstruction.

    Sampling functionality has not yet been implemented; however, we are certain that the dataset is equally suitable for training generative models. The dataset will likely not be a limitation for our task.

    The biggest challenge but also the biggest advantage of the ImageNet dataset is its size. The size imposes a challenge in terms of assessing the data, i.e. the shapes, properties of indivisual classes and their distribution. It is impossable to look at all the images, so we had to draw conclusions from sample image inspection and statistical analysis.

    On the other hand, the size and diversity of the data is a huge advantage for training the models. The models will be able to learn a wide range of features and patterns, and to generalize it to unseen data.


\subsection{A prospect on further Experiements}\label{subsec:further-ideas}
    We utilize the structural similarity index as a metric for image similarity to evaluate the reconstruction quality of our models \ref{subsec:compression}. For comparison to the paper, we still use \ac{mse} loss for training our \ac{ae}. Though, as discussed in \ref{subsec:compression}, \ac{mse} might not be the optimal metric in terms of human perception.

    \ac{ssim} is a metric for precisely this purpose, so we think our models might benifit from using it as a loss function instead of \ac{mse}. In ~\cite{ssim_as_loss}, it is shown that using a loss function trageted to human perception can improve the quality of convolutional neural networks for image reconstruction, in particular using \ac{ssim}. 

\subsection{Working Title: Final words}\label{subsec:final-words}
    Image reconstrution as well as image generation are two realitively well researched topics. Presumably, because of their wide range of applications. While the most prominent methods, i.e. \ac{ae} and \ac{vae}, are two simple yet powerful models, they still have their limitations. 

    \ac{ae} are powerful reconstructors, but they are not able to generate new images, because of their deterministic nature and their discrete latent representation.

    \ac{vae} can sample in the latent space and generate new images by virtue of their stochastic properties. Yet, you sometime favour a discrete latent over latent distributions, because of their simplicity, interpretability and compatibilty with other modalities ~\cite{vqvae}.

    \ac{vq} is an attempt to fill that gap. It enables to reconstruct images, generate new samples while having a discrete latent representation.

\vskip 1em
\textbf{A Note on Overfitting:}
Overfitting refers to the phenomenon where a model starts to memorize the training data instead of learning useful
features, in turn not generalizing well to unseen data.

While theoretically, this may very well be an issue with \ac{ae} and \ac{vae}, we have not experienced this happening
yet with either when training with adequate training-set sizes.
This is likely due to the large dataset sizes we use and the inductive bias of the convolution operation~
\cite{citationNeeded}.